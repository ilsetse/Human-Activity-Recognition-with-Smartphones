# Question 1  

[Code here](https://github.com/ilsetse/kaggle-projects/blob/master/human%20activity%20recognition.ipynb)  
I’ve completed a multiclass classification problem with the Human Activity Recognition with Smartphones dataset from Kaggle. It had a 561-feature vector, and six activity labels. I read the relevant paper by Anguita et al. (2012), where the authors used a support vector machine (SVM) with fixed-point arithmetic to classify the activities. Not all the subjects have the same number of motion data recorded (the average number of entries per subject is around 245), and some subjects’ data are missing. While there are a few missing subjects, there doesn’t look like there are outliers, so I began to follow their methodology to reproduce their results, with the exception of exploiting fixed-point arithmetic, since I didn’t have to run the algorithm on a mobile phone. I then used a recurrent neural network (RNN) with Long Short-Term Memory (LSTM) on Keras to classify the activities, and compared the results to using a SVM. I decided to use a RNN since it accepts a vector of features per time step, which seemed to represent a vector of movement features per activity well, and LSTM avoids the vanishing gradient problem.   
I first used scikit-learn’s SVM module; following the paper, I used a RBF kernel, using grid search to find C and gamma, the hyperparameter pair chosen with lowest cross-validation (k=10) error. I achieved a 96.4% accuracy (96.5% precision) on the test set, compared to 93% accuracy using the default RBF kernel without any settings. The normalized confusion matrix showed that while the paper correctly classified 'laying' 100%, I had 100% accuracy with 'walking', and 'sitting' as a close second at 99.6%. Similar to the paper, 'walking upstairs' and 'walking downstairs' are misclassified more as each other than other activities, likely due to the nature of the action. With the exception of 'laying', all static action performed better than movement actions.  
To build the RNN, I specified the input shape to take a time step of 2, looking at 281 features at each step, and achieved 93.3% accuracy. As with SVM, the normalized confusion matrix showed that walking and sitting was most accurately classified, with 100% accuracy for 'walking'. Here the model had more trouble distinguishing between 'walking upstairs' and 'walking downstairs'. I chose to train the model by looking at more features while advancing by 2 entries at each time step, since I thought that the features were more important. I did experiment with a larger time step of 281, looking at 2 features at each step, but this led to a lower classification accuracy of 82%, which confirms my thoughts.  
In the end, SVM outperformed RNN in my experiment. I think the quality of the results are respectable for a learning exercise since it supported the original paper’s findings. I’d like to see how closely I could reproduce the paper’s results, as well as Guillaume Chevalier’s project results (he achieved ~94% accuracy) by using the raw data from the UCI repository instead of Kaggle’s.  


# Question 2  
First, I need to clean the data and see how much missing data there are. I’d expect to see missing static features, and if there aren’t too many of such entries, I might choose to remove the record, since > 10 million records is a large enough dataset to work with. Then, I would perform exploratory analysis and visualize the data to look at the usage event per month for each active user, and check for outliers or unique behaviors. I’d perform feature engineering if some features are highly correlated, or aggregate some action fields for more data, and transform the data into numerical values. Then, I’d randomly split the data into a 70/30 train/test set. When building the model, I’d first test with all the users, and then separate the users by demographics and compare them for insights. Making separate models for customers who have used the service for more than 2 months may yield significantly different churn rates than new customers who have used the service for 1 month. Churn rate changes with customer growth, and according to Shopify, newer customers tend to churn more than mature customers.  
There seems to be quite a class imbalance with the churn rate at 5%. I’d try two ensemble approaches as a binary-classification problem to classify the data using a balanced data, and also using the representative (imbalanced) data. If it hasn’t been created, I’d create a churn (1), or no churn (0) label for each user and then predict whether or not a user would stop usage in the next 30 days (churned).  
I’d first use a random forest, since it’s less resistant to overfitting and dominating features. It’s also sensitive to imbalanced data, so I would try to balance the data by oversampling the churned sample. Oversampling poses the risk of overfitting the data, but this may be mitigated by using a random forest.   
To classify with the imbalanced data, I would use a Gradient Boosting Classifier that would construct training sets with subsequent misclassifications to learn from the rarer cases at the next iteration. 
A property of the classifiers is that random forests decrease the variance of the model, and boosting decreases the bias of the model. Although oversampling the data may decrease the variance of the data, there may be large enough of a dataset to mitigate this. However, with boosting, if I start with a dataset with very high variance, I risk overfitting the model. So, I must use cross-validation to tune my hyper parameters, and see if the large dataset mitigates the risk of overfitting. These two algorithms could be computationally intensive and take a lot of time to compute, especially with a large dataset.  
I should also evaluate the model preferring the Area Under the ROC curve, precision, and recall since they’re not affected by class imbalance.
With these two approaches, then the problem of the imbalanced class can be explored and the model that maximizes performances could be then kept.  

# Question 3  
- I am driven and I persevere through difficult tasks, which is a useful trait to have when dealing with ever evolving cybersecurity problems. If a subject is difficult to grasp, I would find example problem sets and try to understand how to get to the solution. Even if I don’t succeed the first time, I try not to dwell on the failures and I apply what I learned for the next time.  
- I work well in a collaborative setting. As a Team Lead in AIESEC Helsinki, I strive to keep communication open and encourage my team members to ask questions to avoid any miscommunications, and I always ask my supervisor for clarification and relay the information back to my team so that everyone knows what is going on. When my team has disagreeing opinions, I respect all opinions and try to compromise where I can.  
- I love to learn and I invest my time into reading, listening to podcasts, and attending talks so I could educate myself on issues outside of data science. I’m aware of subjects I’m not strong at, and I read blogs about it so that I can improve. I think this is a useful quality to bring to the team since I can provide a different perspective to problems that require creativity.  
  
- Since my thesis project would be on a more specialized topic, I hope my project will be more machine learning intensive. I’ve found that in many projects, I spend more time on other data science tasks, such as data cleaning and transformation, than on the models.  I would also like to learn in practice how to tell if a model is "right" as opposed to just performing well, and learn more feature engineering techniques. I also want to learn how quality production code for machine learning projects looks like, and how to maintain it.  
- Specific to cybersecurity, I’d like to learn how experienced attackers tend to think and how they discover weaknesses to exploit them. I would also like to learn how security engineers prepare for software and hardware attacks.  
- While I’m not necessarily afraid to ask questions about what I’m uncertain about, it’s a continuous learning process for me to ask good questions, especially ones that are domain specific. I hope I can take advantage of the expertise of my colleagues by asking the right questions.  
